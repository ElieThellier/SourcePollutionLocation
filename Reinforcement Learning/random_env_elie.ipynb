{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plume tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State, S = [x_t, y_t, c_t]\n",
    "* Transition State, S_t = [x_(t+1), y_(t+1), c_(t+1) + Noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import spdiags\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class EnvGeneration:\n",
    "    def __init__(self, grid_size = 50, D = 0.2, velocity_vector = [lambda t: 3.0, lambda t: 2.0], source_pos = [-1.6, -1.6], \n",
    "                source_strength = 1, source_spread = 15, Lx=4, Ly=4, Lt=2, Nt=2000):\n",
    "        self.n = grid_size\n",
    "        self.Nt = Nt\n",
    "        \n",
    "        self.Lx = Lx\n",
    "        self.Ly = Ly\n",
    "        self.Lt = Lt\n",
    "\n",
    "        self.dx = Lx / (grid_size - 1)\n",
    "        self.dy = Ly / (grid_size - 1)\n",
    "        self.dt = Lt / (Nt - 1)\n",
    "        self.x = np.linspace(-Lx / 2, Lx / 2, grid_size)\n",
    "        self.y = np.linspace(-Ly / 2, Ly / 2, grid_size)\n",
    "        self.t = np.linspace(0, Lt, Nt)\n",
    "        self.X, self.Y = np.meshgrid(self.x, self.y)\n",
    "\n",
    "        self.D = D\n",
    "        self.V_x, self.V_y = velocity_vector\n",
    "        self.source_pos = source_pos\n",
    "        self.source_strength = source_strength\n",
    "        self.source_spread = source_spread\n",
    "\n",
    "        self.u_new = np.zeros((grid_size, grid_size))\n",
    "        self.u_old = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    def init_solution(self):\n",
    "        self.f = np.zeros((self.n, self.n))\n",
    "        for i in range(1, self.n-1):\n",
    "            for j in range(1, self.n-1):\n",
    "                self.f[i, j] = self.source_strength * np.exp(-self.source_spread * (((self.x[i] - self.source_pos[0]))**2 + ((self.y[j] - self.source_pos[1]))**2))\n",
    "                \"\"\" if np.random.rand() < 0.1:\n",
    "                    self.f[i, j] = 0\n",
    "                else:\n",
    "                    self.f[i, j] += np.random.normal(loc=0, scale=self.f[i, j] * 0.2) \"\"\"\n",
    "\n",
    "        self.u_vec = np.zeros((self.n * self.n))\n",
    "        self.u_new = self.u_new.flatten()\n",
    "        self.u_old = self.u_old.flatten()\n",
    "        self.f_vec = self.f.flatten()\n",
    "\n",
    "        self.E = self.sparseE(self.n)\n",
    "        self.e = np.ones(self.n * self.n)\n",
    "\n",
    "    def sparseE(self, n):\n",
    "        total_nodes = n * n\n",
    "        diagonals = np.zeros((5, total_nodes))\n",
    "        main_diag = np.ones(total_nodes)\n",
    "        diagonals[2, :] = main_diag\n",
    "        upper_diag = np.ones(total_nodes - n)\n",
    "        diagonals[3, :-n] = upper_diag\n",
    "        lower_diag = np.ones(total_nodes - n)\n",
    "        diagonals[1, n:] = lower_diag\n",
    "        left_diag = np.ones(total_nodes - 1)\n",
    "        left_diag[np.arange(1, total_nodes) % n == 0] = 0\n",
    "        diagonals[0, 1:] = left_diag\n",
    "        right_diag = np.ones(total_nodes - 1)\n",
    "        right_diag[np.arange(total_nodes - 1) % n == n - 1] = 0\n",
    "        diagonals[4, :-1] = right_diag\n",
    "        offsets = [-n, -1, 0, 1, n]\n",
    "        return spdiags(diagonals, offsets, total_nodes, total_nodes, format='csr')\n",
    "\n",
    "    def C_Derivative(self, t, u_vec):\n",
    "        D_fluct = self.D + np.random.normal(loc=0, scale=0.2)\n",
    "        alpha_y = D_fluct / (self.dy**2) - self.V_y(t) / (2 * self.dy)\n",
    "        alpha_x = D_fluct / (self.dx**2) - self.V_x(t) / (2 * self.dx)\n",
    "        beta = - (2 * D_fluct / (self.dx**2) + 2 * self.D / (self.dy**2))\n",
    "        gamma_x = D_fluct / (self.dx**2) + self.V_x(t) / (2 * self.dx)\n",
    "        gamma_y = D_fluct / (self.dy**2) + self.V_y(t) / (2 * self.dy)\n",
    "\n",
    "        row = [gamma_y * self.e, gamma_x * self.e, beta * self.e, alpha_x * self.e, alpha_y * self.e]\n",
    "        diags = [-self.n, -1, 0, 1, self.n]\n",
    "\n",
    "        A = spdiags(row, diags, self.n * self.n, self.n * self.n, format='csr')\n",
    "\n",
    "        k = A.dot(u_vec) + self.f_vec\n",
    "        return k\n",
    "\n",
    "    def solve(self):\n",
    "        for n in range(self.Nt - 1):\n",
    "            k1 = self.C_Derivative(self.t[n], self.u_old)\n",
    "\n",
    "            # Apply zero Dirichlet boundary conditions to k1\n",
    "            k1[0:self.n] = 0  # Top boundary\n",
    "            k1[-self.n:] = 0  # Bottom boundary\n",
    "            k1[::self.n] = 0  # Left boundary\n",
    "            k1[self.n - 1::self.n] = 0  # Right boundary\n",
    "\n",
    "            k2 = self.C_Derivative(self.t[n] + self.dt / 2, self.u_old + k1 * self.dt / 2)\n",
    "\n",
    "            # Apply zero Dirichlet boundary conditions to k2\n",
    "            k2[0:self.n] = 0  # Top boundary\n",
    "            k2[-self.n:] = 0  # Bottom boundary\n",
    "            k2[::self.n] = 0  # Left boundary\n",
    "            k2[self.n - 1::self.n] = 0  # Right boundary\n",
    "\n",
    "            k3 = self.C_Derivative(self.t[n] + self.dt / 2, self.u_old + k2 * self.dt / 2)\n",
    "\n",
    "            # Apply zero Dirichlet boundary conditions to k3\n",
    "            k3[0:self.n] = 0  # Top boundary\n",
    "            k3[-self.n:] = 0  # Bottom boundary\n",
    "            k3[::self.n] = 0  # Left boundary\n",
    "            k3[self.n - 1::self.n] = 0  # Right boundary\n",
    "\n",
    "            k4 = self.C_Derivative(self.t[n] + self.dt, self.u_old + k3 * self.dt)\n",
    "\n",
    "            # Apply zero Dirichlet boundary conditions to k4\n",
    "            k4[0:self.n] = 0  # Top boundary\n",
    "            k4[-self.n:] = 0  # Bottom boundary\n",
    "            k4[::self.n] = 0  # Left boundary\n",
    "            k4[self.n - 1::self.n] = 0  # Right boundary\n",
    "\n",
    "            self.u_new = self.u_old + (1 / 6) * self.dt * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "            self.u_old = self.u_new\n",
    "\n",
    "        self.u_vec = self.u_new\n",
    "        self.u = self.u_vec.reshape(self.n, self.n)\n",
    "        self.u[self.u < 0] = 0\n",
    "        \n",
    "        # spread the solution to the edges\n",
    "        self.u[0, :] = self.u[1, :] * 0.5\n",
    "        self.u[-1, :] = self.u[-2, :] * 0.5\n",
    "        self.u[:, 0] = self.u[:, 1] * 0.5\n",
    "        self.u[:, -1] = self.u[:, -2] * 0.5\n",
    "        \n",
    "        if np.max(self.u) > 100:\n",
    "            raise RuntimeError(\"Simulation diverged.\")\n",
    "\n",
    "    def plot_solution(self):\n",
    "        plt.imshow(self.u, extent=[-self.Lx/2, self.Lx/2, -self.Ly/2, self.Ly/2], origin='lower', cmap='jet')\n",
    "        plt.colorbar()\n",
    "        # add the source position\n",
    "        plt.scatter(self.source_pos[1], self.source_pos[0], c='r', marker='x')\n",
    "        plt.show()\n",
    "    \n",
    "    def scale(self, max_val):\n",
    "        self.u = max_val * (self.u - self.u.min()) / (self.u.max() - self.u.min())\n",
    "        self.u = np.round(self.u)\n",
    "    \n",
    "    def random_source_positon(self):\n",
    "        # random inside the grid but not too close to the edges\n",
    "        self.source_pos = [random.uniform(-1.5, 1.5), random.uniform(-1.5, 1.5)]\n",
    "        \n",
    "    def random_source_strength(self):\n",
    "        self.source_strength = random.uniform(0.9, 1)\n",
    "        \n",
    "    def random_source_spread(self):\n",
    "        self.source_spread = random.uniform(15, 25)\n",
    "        \n",
    "    def random_D(self):\n",
    "        self.D = random.uniform(0.2, 0.3)\n",
    "        \n",
    "    def random_velocity_vector(self):\n",
    "        if self.source_pos[1] < 0 and self.source_pos[0] < 0:\n",
    "            # Source in bottom left corner, velocity should point towards top right\n",
    "            random_x = random.uniform(0, 4) * self.n / 20\n",
    "            random_y = random.uniform(0, 4) * self.n / 20\n",
    "        elif self.source_pos[1] > 0 and self.source_pos[0] > 0:\n",
    "            # Source in top right corner, velocity should point towards bottom left\n",
    "            random_x = random.uniform(-4, 0) * self.n / 20\n",
    "            random_y = random.uniform(-4, 0) * self.n / 20\n",
    "        elif self.source_pos[1] < 0 and self.source_pos[0] > 0:\n",
    "            # Source in top left corner, velocity should point towards bottom right\n",
    "            random_x = random.uniform(0, 4) * self.n / 20\n",
    "            random_y = random.uniform(-4, 0) * self.n / 20\n",
    "        else:\n",
    "            # Source in bottom right corner, velocity should point towards top left\n",
    "            random_x = random.uniform(-4, 0) * self.n / 20\n",
    "            random_y = random.uniform(0, 4) * self.n / 20\n",
    "\n",
    "        self.V_x = lambda t: random_x\n",
    "        self.V_y = lambda t: random_y\n",
    "    \n",
    "    def random_time(self):\n",
    "        self.Lt = random.uniform(0.5, 5)\n",
    "        self.t = np.linspace(0, self.Lt, self.Nt)\n",
    "        \n",
    "    def print_parameters(self):\n",
    "        print(f\"Grid size: {self.n}\")\n",
    "        print(f\"D: {self.D}\")\n",
    "        print(f\"Velocity vector: {self.V_x(0)}, {self.V_y(0)}\")\n",
    "        print(f\"Source position: {[self.source_pos[1], self.source_pos[0]]}\")\n",
    "        print(f\"Source strength: {self.source_strength}\")\n",
    "        print(f\"Source spread: {self.source_spread}\")\n",
    "        print(f\"Time: {self.Lt}\")\n",
    "    \n",
    "    def save_solution(self, path, filename):\n",
    "        np.save(path + filename, self.u)\n",
    "    \n",
    "    def save_image(self, path, filename):\n",
    "        plt.imshow(self.u, extent=[-self.Lx/2, self.Lx/2, -self.Ly/2, self.Ly/2], origin='lower', cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.savefig(path + filename + \".png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def add_noise(self, noise_level):\n",
    "        self.u += np.random.normal(loc=0, scale=np.mean(self.u) * noise_level, size=(self.n, self.n))\n",
    "        self.u[self.u < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MySim(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MySim, self).__init__()\n",
    "        env = EnvGeneration(Nt=500, grid_size=20)\n",
    "        env.random_source_positon()\n",
    "        env.random_source_strength()\n",
    "        env.random_source_spread()\n",
    "        env.random_D()\n",
    "        env.random_velocity_vector()\n",
    "        env.random_time()\n",
    "        env.init_solution()\n",
    "        #env.print_parameters()\n",
    "        env.solve()\n",
    "        # TODO: env.add_noise(0.1)\n",
    "        #env.plot_solution()\n",
    "        self.maze = np.array(env.u)\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        # Plume model information\n",
    "        # transform the source position to the grid using the grid size and domain size, env.n is the grid size, env.Lx is the domain size, env.source_pos is the source position as a scalar value in the domain not in the grid\n",
    "        self.source_pos = env.n * (env.source_pos[0] + env.Lx/2) / env.Lx, env.n * (env.source_pos[1] + env.Ly/2) / env.Ly\n",
    "        # round the source position to the nearest integer using round function\n",
    "        self.source_pos = np.array([round(self.source_pos[0]), round(self.source_pos[1])], dtype=np.int64)\n",
    "        self.r0 = self.source_pos\n",
    "        self.sigma = env.source_spread\n",
    "        self.domain_length = env.Lx\n",
    "        self.domain_width = env.Ly\n",
    "        self.D = env.D\n",
    "        self.V_x = env.V_x(0)\n",
    "        self.V_y = env.V_y(0)\n",
    "\n",
    "        # Training steps information\n",
    "        self.total_step = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # Starting position is the other side of the grid from the source\n",
    "        self.start_pos = np.array([np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)])\n",
    "        if self.start_pos[0] < 0:\n",
    "            self.start_pos[0] = 0\n",
    "        if self.start_pos[1] < 0:\n",
    "            self.start_pos[1] = 0\n",
    "        if self.start_pos[0] > self.num_rows-1:\n",
    "            self.start_pos[0] = self.num_rows-1\n",
    "        if self.start_pos[1] > self.num_cols-1:\n",
    "            self.start_pos[1] = self.num_cols-1\n",
    "        self.start_pos = np.array(self.start_pos, dtype=np.int64)\n",
    "        self.visited = np.array([self.start_pos])\n",
    "\n",
    "        self.current_pos = self.start_pos\n",
    "        self.previous_pos = self.current_pos\n",
    "\n",
    "        # Observation space and action space\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0]), \n",
    "                                            high= np.array([self.num_rows-1, self.num_cols-1]),\n",
    "                                            shape=(2,),\n",
    "                                            dtype=np.int64)\n",
    "        \n",
    "        # Render related\n",
    "        pygame.init()\n",
    "        self.cell_size = int(500/self.num_rows)\n",
    "        self.screen = pygame.display.set_mode((self.num_cols * self.cell_size, self.num_rows * self.cell_size))\n",
    "        \n",
    "        self.action = 0\n",
    "\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.total_episode = 0\n",
    "\n",
    "    def reset(self,**kwargs):\n",
    "        # reset the environment every 10 episodes\n",
    "        #if self.total_episode % 10 == 0:\n",
    "        env = EnvGeneration(Nt=500, grid_size=20)\n",
    "        env.random_source_positon()\n",
    "        env.random_source_strength()\n",
    "        env.random_source_spread()\n",
    "        env.random_D()\n",
    "        env.random_velocity_vector()\n",
    "        env.random_time()\n",
    "        env.init_solution()\n",
    "        #env.print_parameters()\n",
    "        env.solve()\n",
    "        # TODO: env.add_noise(0.1)\n",
    "        #env.plot_solution()\n",
    "        \n",
    "        self.maze = np.array(env.u)\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "    \n",
    "        self.total_episode = 0\n",
    "        \"\"\" else:\n",
    "            self.total_episode += 1 \"\"\"\n",
    "        \n",
    "        # Plume model information\n",
    "        # transform the source position to the grid using the grid size and domain size, env.n is the grid size, env.Lx is the domain size, env.source_pos is the source position as a scalar value in the domain not in the grid\n",
    "        self.source_pos = env.n * (env.source_pos[0] + env.Lx/2) / env.Lx, env.n * (env.source_pos[1] + env.Ly/2) / env.Ly\n",
    "        # round the source position to the nearest integer using round function\n",
    "        self.source_pos = np.array([round(self.source_pos[0]), round(self.source_pos[1])], dtype=np.int64)\n",
    "        self.r0 = self.source_pos\n",
    "        self.sigma = env.source_spread\n",
    "        self.domain_length = env.Lx\n",
    "        self.domain_width = env.Ly\n",
    "        self.D = env.D\n",
    "        self.V_x = env.V_x(0)\n",
    "        self.V_y = env.V_y(0)\n",
    "        \n",
    "        # random starting position\n",
    "        self.start_pos = np.array([np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)])\n",
    "        if self.start_pos[0] < 0:\n",
    "            self.start_pos[0] = 0\n",
    "        if self.start_pos[1] < 0:\n",
    "            self.start_pos[1] = 0\n",
    "        if self.start_pos[0] > self.num_rows-1:\n",
    "            self.start_pos[0] = self.num_rows-1\n",
    "        if self.start_pos[1] > self.num_cols-1:\n",
    "            self.start_pos[1] = self.num_cols-1\n",
    "        self.start_pos = np.array(self.start_pos, dtype=np.int64)\n",
    "        \n",
    "        self.current_pos = self.start_pos\n",
    "        self.previous_pos = self.current_pos\n",
    "        self.visited = np.array([self.start_pos])\n",
    "        self.total_step = 0\n",
    "        self.total_reward = 0\n",
    "        self.render()\n",
    "        return self.current_pos, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move the agent based on the selected action\n",
    "        new_pos = np.array(self.current_pos)\n",
    "        if action == 0:  # Up\n",
    "            new_pos[0] += 1\n",
    "        elif action == 1:  # Down\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 2:  # Left\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 3:  # Right\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        max_pos = np.unravel_index(np.argmax(self.maze, axis=None), self.maze.shape)\n",
    "        # Check if the new position is valid\n",
    "        if self._is_valid_position(new_pos):\n",
    "            #reward = 1000 * self._compute_reward_2(self.current_pos, action)\n",
    "            normalized_maze = self.maze / np.max(self.maze) * 10\n",
    "            reward = normalized_maze[self.current_pos[0], self.current_pos[1]] - 1\n",
    "            # reward = (normalized_maze[new_pos[0], new_pos[1]] - normalized_maze[self.current_pos[0], self.current_pos[1]])\n",
    "            # reward = 1000 * (self.maze[new_pos[0], new_pos[1]] - self.maze[self.current_pos[0], self.current_pos[1]])\n",
    "            reward -= 1 if self._is_visited(new_pos) else 0\n",
    "            self.previous_pos = self.current_pos\n",
    "            self.current_pos = new_pos\n",
    "            self.visited = np.append(self.visited, [new_pos], axis=0)\n",
    "            # reward of 20 if the agent reaches the highest concentration\n",
    "            # get the position of the highest concentration\n",
    "            if np.array_equal(self.current_pos, max_pos):\n",
    "                reward += 20\n",
    "        else:\n",
    "            # The agent collides obstacles or moves out of the domain\n",
    "            reward = -5.0\n",
    "        \n",
    "        done = (self.total_step > 200) or (np.linalg.norm(self.current_pos - max_pos) < 1)\n",
    "        self.total_step += 1\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "        self.action = action\n",
    "        \n",
    "        return self.current_pos, reward, bool(done), False, {}\n",
    "\n",
    "    def _compute_physical_reward(self,pos, action):\n",
    "        # Length and width of each grid\n",
    "        grid_length = self.domain_length / self.num_cols\n",
    "        grid_width = self.domain_width / self.num_rows\n",
    "\n",
    "        # The structure for runge-kutta method\n",
    "        #   2\n",
    "        #1  0  3\n",
    "        #   4\n",
    "        # left: 1, right: 3, up: 2, down: 4, center: 0\n",
    "        # concentration of left, right, up, down, center without using self.r0\n",
    "        concentration = np.zeros(5)\n",
    "        concentration[0] = self.maze[pos[0], pos[1]]\n",
    "        # left\n",
    "        if pos[1] > 0:\n",
    "            concentration[1] = self.maze[pos[0], pos[1] - 1]\n",
    "        else:\n",
    "            concentration[1] = self.maze[pos[0], pos[1]]\n",
    "        # right\n",
    "        if pos[1] < self.num_cols-1:\n",
    "            concentration[3] = self.maze[pos[0], pos[1] + 1]\n",
    "        else:\n",
    "            concentration[3] = self.maze[pos[0], pos[1]]\n",
    "        # up\n",
    "        if pos[0] > 0:\n",
    "            concentration[2] = self.maze[pos[0] - 1, pos[1]]\n",
    "        else:\n",
    "            concentration[2] = self.maze[pos[0], pos[1]]\n",
    "        # down\n",
    "        if pos[0] < self.num_rows-1:\n",
    "            concentration[4] = self.maze[pos[0] + 1, pos[1]]\n",
    "        else:\n",
    "            concentration[4] = self.maze[pos[0], pos[1]]\n",
    "\n",
    "        # Calculate the gradient of the concentration\n",
    "        concentration_gradient = np.zeros(2)\n",
    "        \n",
    "        if action == 0:  # Up\n",
    "            concentration_gradient[0] = 0\n",
    "            concentration_gradient[1] = (concentration[2] - concentration[0]) / (grid_length)\n",
    "        elif action == 1:  # Down\n",
    "            concentration_gradient[0] = 0\n",
    "            concentration_gradient[1] = (concentration[4] - concentration[0]) / (grid_length)\n",
    "        elif action == 2:  # Left\n",
    "            concentration_gradient[0] = (concentration[1] - concentration[0]) / (grid_width)\n",
    "            concentration_gradient[1] = 0\n",
    "        elif action == 3:  # Right\n",
    "            concentration_gradient[0] = (concentration[3] - concentration[0]) / (grid_width)\n",
    "            concentration_gradient[1] = 0\n",
    "        else:\n",
    "            concentration_gradient[0] = 0\n",
    "            concentration_gradient[1] = 0\n",
    "\n",
    "        # Calculate the laplacian of the concentration\n",
    "        concentration_laplacian = (concentration[3] - 2 * concentration[0] + concentration[1]) / (grid_length ** 2) + (concentration[4] - 2 * concentration[0] + concentration[2]) / (grid_width ** 2)\n",
    "        # Calculate physical reward using the concentration gradient and laplacian\n",
    "        #TODO: reward_physical = -self.D * concentration_laplacian + self.V_x * concentration_gradient[0] + self.V_y * concentration_gradient[1]\n",
    "        \n",
    "        reward_physical = np.abs(self.V_x) * concentration_gradient[0] + np.abs(self.V_y) * concentration_gradient[1]\n",
    "        \n",
    "        # print(-self.D * concentration_laplacian, self.V_x * concentration_gradient[0], self.V_y * concentration_gradient[1])\n",
    "\n",
    "        return reward_physical\n",
    "    \n",
    "    def _compute_reward_2(self, pos, action):\n",
    "        gamma = 0.1\n",
    "\n",
    "        # current concentration\n",
    "        current_concentration = self.maze[pos[0], pos[1]]\n",
    "        # concentration of the previous position\n",
    "        \"\"\" previous_concentration = self.maze[self.previous_pos[0], self.previous_pos[1]]\n",
    "\n",
    "        if current_concentration > previous_concentration:\n",
    "            is_movement_towards_source = True\n",
    "        else:\n",
    "            is_movement_towards_source = False\n",
    "        \n",
    "        # Reward for movement towards the pollution source\n",
    "        r_movement = 0.5 if is_movement_towards_source else -0. \"\"\"\n",
    "    \n",
    "        # Proximity reward (if needed)\n",
    "        #TODO r_proximity = np.exp(-gamma * np.linalg.norm(current_concentration - previous_concentration))\n",
    "\n",
    "        \"\"\" # current position\n",
    "        pos_x = self.domain_length * (pos[0] - 0.5) / self.num_cols - self.domain_length / 2\n",
    "        pos_y = self.domain_width * (pos[1] - 0.5) / self.num_rows - self.domain_width / 2\n",
    "\n",
    "        if pos_x == 0 and pos_y == 0:\n",
    "            is_hit_boundary = True\n",
    "        else:\n",
    "            is_hit_boundary = False\n",
    "        \n",
    "        # Penalty for hitting the boundaries of the grid\n",
    "        r_boundary = -0.1 if is_hit_boundary else 0.0 \"\"\"\n",
    "\n",
    "        # Pyshical reward\n",
    "        r_pi = self._compute_physical_reward(pos, action)\n",
    "\n",
    "        # visited reward\n",
    "        r_visited = 0.0\n",
    "        if self._is_visited(pos):\n",
    "            r_visited = -5\n",
    "\n",
    "        # Calculate total reward\n",
    "        total_reward = r_pi # + r_visited # + r_movement + r_boundary + r_proximity\n",
    "    \n",
    "        # Print the reward components\n",
    "        #print(f\"Reward for movement towards source: {r_movement}\")\n",
    "        #print(f\"Proximity reward: {r_proximity}\")\n",
    "        #print(f\"Penalty for hitting boundary: {r_boundary}\")\n",
    "        #print(f\"Physical reward: {r_pi}\")\n",
    "        #print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "    def _is_visited(self,pos):\n",
    "        if np.any(np.all(pos == self.visited, axis=1)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _is_valid_position(self, pos):\n",
    "        row, col = pos\n",
    "        # If agent goes out of the grid\n",
    "        if row < 0 or col < 0 or row >= self.num_rows or col >= self.num_cols:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def render(self):\n",
    "        self.screen.fill((255,255,255))\n",
    "        # Draw env elements one cell at a time\n",
    "\n",
    "        for row in range(self.num_rows - 1, -1, -1):  # Iterate over rows in reverse order\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = (self.num_rows - row - 1) * self.cell_size  # Calculate cell_top inverted\n",
    "\n",
    "                # Draw the grid\n",
    "                pygame.draw.rect(self.screen, (0, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size), 1)\n",
    "\n",
    "                # draw the pollution concentration\n",
    "                color = cm.jet(self.maze[row, col]/np.max(self.maze))\n",
    "                pygame.draw.rect(self.screen, [int(255*color[0]), int(255*color[1]), int(255*color[2])], (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "                # Draw visited block with gray color\n",
    "                # if self._is_visited([row,col]):\n",
    "                #     pygame.draw.rect(self.screen, [150,150,150], (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "                # starting position\n",
    "                if row == self.start_pos[0] and col == self.start_pos[1]:\n",
    "                    pygame.draw.rect(self.screen, (0, 255, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "                if np.array_equal(np.array(self.current_pos), np.array([row, col])):  # Agent position\n",
    "                    pygame.draw.circle(self.screen, (255, 255, 255), (cell_left+self.cell_size/2, cell_top+self.cell_size/2),self.cell_size/2)\n",
    "\n",
    "                if row == self.source_pos[0] and col == self.source_pos[1]:  # Source position\n",
    "                    pygame.draw.circle(self.screen, (0, 0, 0), (cell_left+self.cell_size/2, cell_top+self.cell_size/2),self.cell_size/2)\n",
    "        # Plot the path of visited positions with lines\n",
    "        # The same function with \"Draw visited block with gray color\", but more nice-looking\n",
    "        for i in range(0,len(self.visited)-1):\n",
    "            next_cell_left = self.visited[i][1] * self.cell_size\n",
    "            next_cell_top  = self.visited[i][0] * self.cell_size\n",
    "            previous_cell_left = self.visited[i+1][1] * self.cell_size\n",
    "            previous_cell_top  = self.visited[i+1][0] * self.cell_size\n",
    "            next_coordianate = [next_cell_left+self.cell_size/2, next_cell_top+self.cell_size/2]\n",
    "            previous_coordiante = [previous_cell_left+self.cell_size/2, previous_cell_top+self.cell_size/2]\n",
    "            pygame.draw.line(self.screen, [0,0,0], \n",
    "                                (next_coordianate[0],next_coordianate[1]), \n",
    "                                (previous_coordiante[0],previous_coordiante[1]), 2)\n",
    "        # write the total reward on the screen and the current reward\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        text = font.render(\"Total reward: \" + str(self.total_reward), True, (255, 255, 255))\n",
    "        self.screen.blit(text, (10, 10))\n",
    "\n",
    "        if len(self.rewards) > 0:\n",
    "            text = font.render(\"Current reward: \" + str(self.rewards[-1]), True, (255, 255, 255))\n",
    "            self.screen.blit(text, (10, 40))\n",
    "        \n",
    "        # write the current step on the screen in the bottom right corner\n",
    "        text = font.render(str(self.total_step), True, (255, 255, 255))\n",
    "        self.screen.blit(text, (self.num_cols * self.cell_size - 50, self.num_rows * self.cell_size - 50))\n",
    "        \n",
    "        # write the action on the screen in the bottom left corner\n",
    "        action_text = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
    "        text = font.render(action_text[self.action], True, (255, 255, 255))\n",
    "        self.screen.blit(text, (50, self.num_rows * self.cell_size - 50))\n",
    "        \n",
    "        pygame.display.update()  # Update the display\n",
    "        \n",
    "    def plot_sum_reward(self):\n",
    "        plt.plot(np.cumsum(self.rewards))\n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = MySim()\n",
    "\n",
    "# obs = env.reset()\n",
    "\n",
    "env.render()\n",
    "\n",
    "#check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "class EpisodeRewardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(EpisodeRewardCallback, self).__init__(verbose)\n",
    "        self.total_episode_reward = 0\n",
    "        self.episode_rewards_list = []\n",
    "        self.step = 0\n",
    "        self.episode = 0\n",
    "    def _on_step(self) -> bool:\n",
    "        self.total_episode_reward += self.locals[\"rewards\"]\n",
    "        # print(f\"Episode: {self.episode}, Steps: {self.step}, Total reward:\", self.total_episode_reward)\n",
    "        if self.locals.get(\"dones\"): # or self.locals.get(\"done\"):\n",
    "            self.episode += 1\n",
    "            print(f\"Episode: {self.episode}, Steps: {self.step}, Total reward:\", self.total_episode_reward)\n",
    "            self.episode_rewards_list.append(self.total_episode_reward)\n",
    "            self.step = 0\n",
    "            self.total_episode_reward = 0\n",
    "        else:\n",
    "            self.step += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, env: VecEnv, render_freq: int = 1000):\n",
    "        super(RenderCallback, self).__init__()\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "        self.n_calls = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.n_calls += 1\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total reward: [-438.01947]\n",
      "Episode: 2, Total reward: [287.48917]\n",
      "Episode: 3, Total reward: [-807.0194]\n",
      "Episode: 4, Total reward: [-508.51456]\n",
      "Episode: 5, Total reward: [-260.73047]\n",
      "Episode: 6, Total reward: [-478.84967]\n",
      "Episode: 7, Total reward: [-543.54266]\n",
      "Episode: 8, Total reward: [-373.631]\n",
      "Episode: 9, Total reward: [-433.93506]\n",
      "Episode: 10, Total reward: [45.358505]\n",
      "Episode: 11, Total reward: [-297.9713]\n",
      "Episode: 12, Total reward: [459.52405]\n",
      "Episode: 13, Total reward: [-444.46576]\n",
      "Episode: 14, Total reward: [-705.5806]\n",
      "Episode: 15, Total reward: [426.25378]\n",
      "Episode: 16, Total reward: [-170.17331]\n",
      "Episode: 17, Total reward: [364.41156]\n",
      "Episode: 18, Total reward: [509.7627]\n",
      "Episode: 19, Total reward: [-54.89569]\n",
      "Episode: 20, Total reward: [-58.70429]\n",
      "Episode: 21, Total reward: [-49.004055]\n",
      "Episode: 22, Total reward: [-64.696724]\n",
      "Episode: 23, Total reward: [542.7012]\n",
      "Episode: 24, Total reward: [77.191345]\n",
      "Episode: 25, Total reward: [-132.56818]\n",
      "Episode: 26, Total reward: [92.690125]\n",
      "Episode: 27, Total reward: [-102.368645]\n",
      "Episode: 28, Total reward: [-311.5245]\n",
      "Episode: 29, Total reward: [93.22895]\n",
      "Episode: 30, Total reward: [219.36324]\n",
      "Episode: 31, Total reward: [98.57491]\n",
      "Episode: 32, Total reward: [143.00964]\n",
      "Episode: 33, Total reward: [-157.6453]\n",
      "Episode: 34, Total reward: [-90.975586]\n",
      "Episode: 35, Total reward: [677.4384]\n",
      "Episode: 36, Total reward: [816.0378]\n",
      "Episode: 37, Total reward: [196.7282]\n",
      "Episode: 38, Total reward: [253.69077]\n",
      "Episode: 39, Total reward: [485.5327]\n",
      "Episode: 40, Total reward: [47.18525]\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env=env, verbose=0)\n",
    "\"\"\" # change learning rate\n",
    "model.learning_rate = 0.0001\n",
    "# change batch size\n",
    "model.batch_size = 64\n",
    "# change number of epochs\n",
    "model.n_epochs = 10 \"\"\"\n",
    "model.learn(total_timesteps=20000, callback=[EpisodeRewardCallback(), RenderCallback(env)])\n",
    "\n",
    "model.save(f\"ppo_maze_rewardFn2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m obs, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print({0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}[action.item()])\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#filename = r\"C:\\Users\\Noufalmesafri\\Desktop\\Deep Learning\\Assignment\\images\\screen3_%04d.png\" % (frame_count)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#screen = env.screen\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#pygame.image.save(screen, filename)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(f\"Step: {frame_count}, Current reward: {reward}, Total reward: {env.total_reward}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 321\u001b[0m, in \u001b[0;36mMySim.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mcolor[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mcolor[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m*\u001b[39mcolor[\u001b[38;5;241m2\u001b[39m])], (cell_left, cell_top, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size))\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# Draw visited block with gray color\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_visited\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    322\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, [\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m150\u001b[39m], (cell_left, cell_top, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size))\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# starting position\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 291\u001b[0m, in \u001b[0;36mMySim._is_visited\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_visited\u001b[39m(\u001b[38;5;28mself\u001b[39m,pos):\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisited\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2416\u001b[0m, in \u001b[0;36m_all_dispatcher\u001b[1;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;124;03m    Test whether any array element along a given axis evaluates to True.\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2410\u001b[0m \n\u001b[0;32m   2411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mlogical_or, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   2413\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m-> 2416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_all_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2417\u001b[0m                     where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import pygame\n",
    "\n",
    "# env.close()\n",
    "\n",
    "model = PPO.load(\"ppo_maze_rewardFn2\")\n",
    "\n",
    "env = MySim()\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "frame_count = 0\n",
    "done = False\n",
    "while not done:\n",
    "    pygame.event.get()\n",
    "    action, state = model.predict(observation = obs)#, deterministic=True)\n",
    "\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    \n",
    "    # print({0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}[action.item()])\n",
    "    \n",
    "    env.render()\n",
    "    frame_count += 1\n",
    "    #filename = r\"C:\\Users\\Noufalmesafri\\Desktop\\Deep Learning\\Assignment\\images\\screen3_%04d.png\" % (frame_count)\n",
    "    #screen = env.screen\n",
    "    #pygame.image.save(screen, filename)\n",
    "    \n",
    "    # print(f\"Step: {frame_count}, Current reward: {reward}, Total reward: {env.total_reward}\")\n",
    "    pygame.time.wait(100)\n",
    "\n",
    "print(\"FINISHED\")\n",
    "print(f\"Step: {frame_count}, Total reward: {env.total_reward}\")\n",
    "env.plot_sum_reward()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3.1 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24215.1 for x64\n",
      "  configuration: --prefix=/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../prebuilt/windows/x64 --toolchain=msvc --extra-cflags=-MD --arch=x86_64 --disable-x86asm --disable-iconv --disable-network --enable-filter=stereo3d --enable-libmp3lame --enable-libdav1d --extra-cflags='-I/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../zlib-build/prebuilt/include -I/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../libpng-build/prebuilt/include -I/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../lame-build/prebuilt/include -I/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../libaom-build/prebuilt/include -I/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../libdav1d-build/prebuilt/include' --extra-ldflags='-L/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../lame-build/prebuilt/windows/x64/lib -L/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../libaom-build/prebuilt/windows/x64/lib -L/c/Work/Source/fdm-qml/ffmpeg-build/windows/ffmpeg/../../../libdav1d-build/prebuilt/windows/x64/lib'\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "Input #0, image2, from 'screen_%04d.png':\n",
      "  Duration: 00:01:36.00, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 500x500, 5 fps, 5 tbr, 5 tbn, 5 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> mpeg4 (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to 'window_video.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Video: mpeg4 (mp4v / 0x7634706D), yuv420p, 500x500, q=2-31, 200 kb/s, 5 fps, 10240 tbn, 5 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 mpeg4\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
      "frame=   49 fps=0.0 q=1.6 size=       0kB time=00:00:09.60 bitrate=   0.0kbits/s speed=18.6x    \n",
      "frame=   85 fps= 83 q=1.6 size=       0kB time=00:00:16.80 bitrate=   0.0kbits/s speed=16.4x    \n",
      "frame=  118 fps= 77 q=2.0 size=     256kB time=00:00:23.40 bitrate=  89.6kbits/s speed=15.3x    \n",
      "frame=  152 fps= 75 q=2.0 size=     256kB time=00:00:30.20 bitrate=  69.5kbits/s speed=14.9x    \n",
      "frame=  189 fps= 74 q=2.0 size=     256kB time=00:00:37.60 bitrate=  55.8kbits/s speed=14.8x    \n",
      "frame=  232 fps= 76 q=2.0 size=     512kB time=00:00:46.20 bitrate=  90.8kbits/s speed=15.2x    \n",
      "frame=  269 fps= 76 q=2.0 size=     512kB time=00:00:53.60 bitrate=  78.3kbits/s speed=15.1x    \n",
      "frame=  305 fps= 75 q=2.0 size=     768kB time=00:01:00.80 bitrate= 103.5kbits/s speed=  15x    \n",
      "frame=  348 fps= 76 q=2.0 size=     768kB time=00:01:09.40 bitrate=  90.7kbits/s speed=15.2x    \n",
      "frame=  384 fps= 76 q=2.0 size=     768kB time=00:01:16.60 bitrate=  82.1kbits/s speed=15.2x    \n",
      "frame=  421 fps= 76 q=1.6 size=    1024kB time=00:01:24.00 bitrate=  99.9kbits/s speed=15.1x    \n",
      "frame=  460 fps= 76 q=2.0 size=    1024kB time=00:01:31.80 bitrate=  91.4kbits/s speed=15.1x    \n",
      "frame=  480 fps= 76 q=2.0 Lsize=    1288kB time=00:01:35.80 bitrate= 110.1kbits/s speed=15.2x    \n",
      "video:1285kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.224485%\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -r 5 -f image2 -s 500x500 -i screen_%04d.png window_video.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
